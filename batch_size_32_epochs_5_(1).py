# -*- coding: utf-8 -*-
"""batch-size-32-epochs-5 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dU6VeQJmImUF6tCQOidDWu_2c9VOzMWQ
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import pandas as pd
import torch
import torch.nn as nn
from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

train_data = pd.read_csv('/kaggle/input/truth-seekers/train.csv')

# Load model directly
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("MYC007/Real-and-Fake-News-Detection")
model = AutoModelForSequenceClassification.from_pretrained("MYC007/Real-and-Fake-News-Detection",
                                                          num_labels = 2)

# custom dataset for PyTorch
class CustomDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=128):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = str(self.data.iloc[idx]['text'])
        label = int(self.data.iloc[idx]['label'])
        inputs = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt',
            truncation=True,
        )
        return {
            'input_ids': inputs['input_ids'].flatten(),
            'attention_mask': inputs['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long),
        }

# DataLoader for training
batch_size = 32
train_dataset = CustomDataset(train_data, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model.to(device)
print('model saved to device')

def training_loop(num_epochs, lr):

    # optimizer and learning rate scheduler
    optimizer = AdamW(model.parameters(), lr=lr)
    total_steps = len(train_loader) * num_epochs
    scheduler = get_linear_schedule_with_warmup(optimizer,
                                                num_warmup_steps=0,
                                                num_training_steps=total_steps)

    # loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0

        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            total_loss += loss.item()
            loss.backward()
            optimizer.step()
            scheduler.step()

        avg_train_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch + 1}/{num_epochs} - Average training loss: {avg_train_loss:.4f}")

training_loop(num_epochs = 4, lr = 2e-5)

training_loop(num_epochs = 1, lr = 2e-7)

# save the model
torch.save(model.state_dict(), "myc_model_99pct.pth")

"""## Prediction"""

test_df = pd.read_csv("/kaggle/input/truth-seekers/test.csv")
test_df['label'] = 1 # adding dummy column
test_df.head(2)

infer_dataset = CustomDataset(test_df, tokenizer)
infer_loader = DataLoader(infer_dataset, batch_size=batch_size)

# prediction on test loader
model.eval()
outputs_list_infer = []
labels_list = []

with torch.no_grad():
    for batch in tqdm(infer_loader, desc=f'test subset'):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)

        _, predicted_labels = torch.max(outputs.logits, 1)
        outputs_list_infer.append(predicted_labels)
        labels_list.append(labels)

# make flat list of outputs and labels
losses_np = [x.cpu().numpy() for x in outputs_list_infer]
flat_list_outputs = [item for sublist in losses_np for item in sublist]

len(flat_list_outputs)

# sample sub
sample_sub = pd.read_csv("/kaggle/input/truth-seekers/sample_submission.csv")
print(sample_sub.shape)
sample_sub['label'] = flat_list_outputs

sample_sub = sample_sub.set_index('id')
sample_sub

sample_sub.to_csv('pytorch_myc_finetune_finalsub_batch32_4epochs.csv')

from IPython.display import HTML

sample_sub.to_csv('pytorch_myc_finetune_finalsub_batchsize32.csv')

def create_download_link(title = "Download CSV file", filename = "data.csv"):
    html = '<a href={filename}>{title}</a>'
    html = html.format(title=title,filename=filename)
    return HTML(html)

# create a link to download the dataframe which was saved with .to_csv method
create_download_link(filename='pytorch_myc_finetune_finalsub.csv')

